{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Necessary Libraries and modules","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport PIL\nimport PIL.Image\nimport tensorflow as tf\nimport pathlib\nimport matplotlib.pyplot as plt\nimport cv2\nimport imghdr","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:45.480192Z","iopub.execute_input":"2022-12-07T13:23:45.480579Z","iopub.status.idle":"2022-12-07T13:23:45.485998Z","shell.execute_reply.started":"2022-12-07T13:23:45.480547Z","shell.execute_reply":"2022-12-07T13:23:45.484887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Bath size and Image inputs Dimensions of our Dataset","metadata":{}},{"cell_type":"code","source":"batch_size = 64\nimg_height = 48\nimg_width = 48","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:45.488386Z","iopub.execute_input":"2022-12-07T13:23:45.488780Z","iopub.status.idle":"2022-12-07T13:23:45.496427Z","shell.execute_reply.started":"2022-12-07T13:23:45.488744Z","shell.execute_reply":"2022-12-07T13:23:45.495497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating our Training and Validation Data","metadata":{}},{"cell_type":"code","source":"train_ds = tf.keras.utils.image_dataset_from_directory(\n \"../input/fer2013/train\",\n  labels = \"inferred\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,\n  color_mode='grayscale',\n  label_mode=\"categorical\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:45.498311Z","iopub.execute_input":"2022-12-07T13:23:45.498903Z","iopub.status.idle":"2022-12-07T13:23:50.931849Z","shell.execute_reply.started":"2022-12-07T13:23:45.498842Z","shell.execute_reply":"2022-12-07T13:23:50.930901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds = tf.keras.utils.image_dataset_from_directory(\n \"../input/fer2013/test\",\n  labels = 'inferred',\n  label_mode = \"categorical\",\n  seed=123,\n  image_size=(48, 48),\n  batch_size=batch_size,\n  color_mode='grayscale')","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:50.934039Z","iopub.execute_input":"2022-12-07T13:23:50.934626Z","iopub.status.idle":"2022-12-07T13:23:52.259210Z","shell.execute_reply.started":"2022-12-07T13:23:50.934587Z","shell.execute_reply":"2022-12-07T13:23:52.258211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Obtaining Classes or Labels of our Training and Validation Dataset","metadata":{}},{"cell_type":"code","source":"class_names = train_ds.class_names\nclass_names","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:52.261772Z","iopub.execute_input":"2022-12-07T13:23:52.262133Z","iopub.status.idle":"2022-12-07T13:23:52.269693Z","shell.execute_reply.started":"2022-12-07T13:23:52.262098Z","shell.execute_reply":"2022-12-07T13:23:52.268537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Obtaining Image_batch and label_batch of our Datasets","metadata":{}},{"cell_type":"code","source":"for image_batch, labels_batch in train_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:52.271492Z","iopub.execute_input":"2022-12-07T13:23:52.272446Z","iopub.status.idle":"2022-12-07T13:23:52.493263Z","shell.execute_reply.started":"2022-12-07T13:23:52.272410Z","shell.execute_reply":"2022-12-07T13:23:52.491376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalizing the Pixel Values of our Datasets\nThis is done so as to regularize the pixel values of the images between 0 and 1.","metadata":{}},{"cell_type":"code","source":"train_ds = train_ds.map(lambda x,y: (x/255, y))\nval_ds = val_ds.map(lambda x,y: (x/255, y))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:52.494465Z","iopub.execute_input":"2022-12-07T13:23:52.494881Z","iopub.status.idle":"2022-12-07T13:23:52.535953Z","shell.execute_reply.started":"2022-12-07T13:23:52.494824Z","shell.execute_reply":"2022-12-07T13:23:52.535062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confirming our Regularized Datasets","metadata":{}},{"cell_type":"code","source":"train_ds.as_numpy_iterator().next()[0].min()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:52.537444Z","iopub.execute_input":"2022-12-07T13:23:52.537804Z","iopub.status.idle":"2022-12-07T13:23:52.771777Z","shell.execute_reply.started":"2022-12-07T13:23:52.537769Z","shell.execute_reply":"2022-12-07T13:23:52.770885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds.as_numpy_iterator().next()[0].max()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:52.773326Z","iopub.execute_input":"2022-12-07T13:23:52.773684Z","iopub.status.idle":"2022-12-07T13:23:52.994701Z","shell.execute_reply.started":"2022-12-07T13:23:52.773649Z","shell.execute_reply":"2022-12-07T13:23:52.993657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuring Dataset for a Better Performance","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:52.998647Z","iopub.execute_input":"2022-12-07T13:23:52.999334Z","iopub.status.idle":"2022-12-07T13:23:53.006811Z","shell.execute_reply.started":"2022-12-07T13:23:52.999296Z","shell.execute_reply":"2022-12-07T13:23:53.005893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a Simple Neural Network\nWe will be creating the first neural network that will consist of 4 block of layers. This will act as a sort of control model. What this means is that we will be comparing it with the second model which will be a tuned one.","metadata":{}},{"cell_type":"code","source":"import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nmodel_1 = keras.Sequential([\n    layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=(img_height, img_width, 1)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(),\n    \n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(),\n    \n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(),\n                  \n    layers.Flatten(),\n    layers.Dense(128, activation=\"relu\"),\n    layers.Dense(7, activation=\"softmax\")\n    ])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:53.008534Z","iopub.execute_input":"2022-12-07T13:23:53.009192Z","iopub.status.idle":"2022-12-07T13:23:53.089178Z","shell.execute_reply.started":"2022-12-07T13:23:53.009146Z","shell.execute_reply":"2022-12-07T13:23:53.088318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compiling our Model\n","metadata":{}},{"cell_type":"code","source":"model_1.compile(\"adam\",\n  loss=\"categorical_crossentropy\",\n    metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:53.090574Z","iopub.execute_input":"2022-12-07T13:23:53.090918Z","iopub.status.idle":"2022-12-07T13:23:53.101733Z","shell.execute_reply.started":"2022-12-07T13:23:53.090884Z","shell.execute_reply":"2022-12-07T13:23:53.100786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary of our Model","metadata":{}},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:53.103454Z","iopub.execute_input":"2022-12-07T13:23:53.103902Z","iopub.status.idle":"2022-12-07T13:23:53.110683Z","shell.execute_reply.started":"2022-12-07T13:23:53.103841Z","shell.execute_reply":"2022-12-07T13:23:53.109553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training our Model","metadata":{}},{"cell_type":"code","source":"history_1 = model_1.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=20,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:23:53.112590Z","iopub.execute_input":"2022-12-07T13:23:53.113412Z","iopub.status.idle":"2022-12-07T13:25:37.196907Z","shell.execute_reply.started":"2022-12-07T13:23:53.113377Z","shell.execute_reply":"2022-12-07T13:25:37.195990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accuracy and Loss Curve","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nhistory_frame_1 = pd.DataFrame(history_1.history)\nhistory_frame_1.loc[:, [\"accuracy\", \"val_accuracy\"]].plot()\nplt.title(\"Accuracy Curve_1\")\nhistory_frame_1.loc[:, [\"loss\", \"val_loss\"]].plot()\nplt.title(\"Loss Curve_1\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:25:37.200090Z","iopub.execute_input":"2022-12-07T13:25:37.200377Z","iopub.status.idle":"2022-12-07T13:25:37.560717Z","shell.execute_reply.started":"2022-12-07T13:25:37.200351Z","shell.execute_reply":"2022-12-07T13:25:37.559791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a tuned Deep Neural Network\nThis neural network consist of 6 block of layers. We did some data augmentation by flipping our image horizontally and thereby creating more examples for our model to learn from and this was done using the `RandomFlip()` function. Also, we had to zoom into our image using the `RandomZoom()` function. The essence of data augmentation is simply to enhance the data passed to the model in such a way that more features from the data is provided to the model for learning.\n\nAlso, we ensure the `padding` parameter is `same` in each of the subsequent `Maoling2D` or `Conv2D` after their first occurence in each blocks when creating such a large neural network so as to avoid `ValueError`.","metadata":{}},{"cell_type":"code","source":"\nmodel_2 = keras.Sequential([\n    layers.RandomFlip(\"horizontal\", input_shape =(img_height, img_width, 1)),\n    layers.RandomZoom(0.2, 0.3),\n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=(2,2), padding='same'),\n    layers.Dropout(0.25),\n    \n    layers.Conv2D(64, (3,3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=(2,2), padding='same'),\n    layers.Dropout(0.25),\n    \n    layers.Conv2D(128, (3,3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=(2,2), padding='same'),\n    layers.Dropout(0.25),\n    \n    layers.Conv2D(128, (3,3), activation='relu'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=(2,2), padding='same'),\n    layers.Dropout(0.25),\n    \n    layers.Conv2D(128, (3,3), activation='relu', padding=\"same\"),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=(2,2), padding='same'),\n    layers.Dropout(0.25),\n                  \n    layers.Flatten(),\n    \n    layers.Dense(256, activation='relu'),\n    layers.Dense(7, activation='softmax'),\n])","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:25:37.563799Z","iopub.execute_input":"2022-12-07T13:25:37.564103Z","iopub.status.idle":"2022-12-07T13:25:37.770815Z","shell.execute_reply.started":"2022-12-07T13:25:37.564076Z","shell.execute_reply":"2022-12-07T13:25:37.769887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compiling our Model\nWe add a learning rate to your network when compiling. The learning rate controls how quickly or slowly a neural network model learns a problem.","metadata":{}},{"cell_type":"code","source":"model_2.compile(\n  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, epsilon=1e-06),\n  loss=\"categorical_crossentropy\",\n    metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:25:37.772181Z","iopub.execute_input":"2022-12-07T13:25:37.773308Z","iopub.status.idle":"2022-12-07T13:25:37.784693Z","shell.execute_reply.started":"2022-12-07T13:25:37.773267Z","shell.execute_reply":"2022-12-07T13:25:37.783730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary of our Model","metadata":{}},{"cell_type":"code","source":"model_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:25:37.786115Z","iopub.execute_input":"2022-12-07T13:25:37.786460Z","iopub.status.idle":"2022-12-07T13:25:37.797722Z","shell.execute_reply.started":"2022-12-07T13:25:37.786422Z","shell.execute_reply":"2022-12-07T13:25:37.796789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training our Model","metadata":{}},{"cell_type":"code","source":"history_2 = model_2.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=120,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:25:37.799095Z","iopub.execute_input":"2022-12-07T13:25:37.799937Z","iopub.status.idle":"2022-12-07T13:35:19.685683Z","shell.execute_reply.started":"2022-12-07T13:25:37.799902Z","shell.execute_reply":"2022-12-07T13:35:19.684721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss and Accuracy Curve","metadata":{}},{"cell_type":"code","source":"\nhistory_frame_2 = pd.DataFrame(history_2.history)\nhistory_frame_2.loc[:, [\"accuracy\", \"val_accuracy\"]].plot()\nplt.title(\"Accuracy Curve_2\")\nhistory_frame_2.loc[:, [\"loss\", \"val_loss\"]].plot()\nplt.title(\"Loss Curve_2\")\nplt.show()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-07T13:35:19.687318Z","iopub.execute_input":"2022-12-07T13:35:19.687977Z","iopub.status.idle":"2022-12-07T13:35:20.033813Z","shell.execute_reply.started":"2022-12-07T13:35:19.687940Z","shell.execute_reply":"2022-12-07T13:35:20.032969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observation\nWe can now see that the newly created model, `model-2`, is a better model in comparison to the first model. ","metadata":{}},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"We evaluate our trained model using our validation dataset. ","metadata":{}},{"cell_type":"code","source":"score = model_2.evaluate(val_ds)\nscore = 100 * round(score[1], 3)\nprint(f\"The accuracy of this model is {score}%\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:35:20.035309Z","iopub.execute_input":"2022-12-07T13:35:20.036266Z","iopub.status.idle":"2022-12-07T13:35:20.490195Z","shell.execute_reply.started":"2022-12-07T13:35:20.036226Z","shell.execute_reply":"2022-12-07T13:35:20.489287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model did well on the validation data with an accuracy of 64.1%.","metadata":{}},{"cell_type":"markdown","source":"## Saving our Model","metadata":{}},{"cell_type":"markdown","source":"We need to save our model should in case we need to make use of it outside this environment.","metadata":{}},{"cell_type":"code","source":"model_1.save(\"emotion_detector.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:35:20.491440Z","iopub.execute_input":"2022-12-07T13:35:20.492419Z","iopub.status.idle":"2022-12-07T13:35:20.547371Z","shell.execute_reply.started":"2022-12-07T13:35:20.492379Z","shell.execute_reply":"2022-12-07T13:35:20.546322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now dowload the emotion_detector.h5 and go to your computer , use this code to test your model","metadata":{}},{"cell_type":"markdown","source":"copy the last cell and execute the code or go to github and download the repository","metadata":{}},{"cell_type":"markdown","source":"github: https://github.com/imenselmi/Emotion-Recognition-using-CNN.git","metadata":{}},{"cell_type":"markdown","source":"dataset drive:https://drive.google.com/drive/folders/1mFOMxCEPBW5vL5JsYf2UZSubGrVH1le8?usp=sharing","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom keras.models import model_from_json\n\n\nemotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n\n# load json and create model\njson_file = open('emotion_model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nemotion_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nemotion_model.load_weights(\"emotion_model.h5\")\nprint(\"Loaded model from disk\")\n\n# start the webcam feed\n#cap = cv2.VideoCapture(0)\n\n# pass here your video path\ncap = cv2.VideoCapture(\"vid.mp4\")\n\nwhile True:\n    # Find haar cascade to draw bounding box around face\n    ret, frame = cap.read()\n    frame = cv2.resize(frame, (1280, 720))\n    if not ret:\n        break\n    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # detect faces available on camera\n    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n\n    # take each face available on the camera and Preprocess it\n    for (x, y, w, h) in num_faces:\n        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n\n        # predict the emotions\n        emotion_prediction = emotion_model.predict(cropped_img)\n        maxindex = int(np.argmax(emotion_prediction))\n        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n\n    cv2.imshow('Emotion Detection', frame)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()","metadata":{},"execution_count":null,"outputs":[]}]}